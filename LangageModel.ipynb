{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =Path('/home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts') #Path to where the repository is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.load(path / 'eswiki/tmp/trn_ids.npy',allow_pickle=True)\n",
    "val_lm = np.load(path/ 'eswiki/tmp/val_ids.npy',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2076502"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = pickle.load(open(path / 'eswiki/tmp/itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_ids(path,vocab,trn_lm, val_lm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/OCR/lib/python3.7/site-packages/fastai/text/learner.py:213: UserWarning: There are no pretrained weights for that architecture yet!\n",
      "  warn(\"There are no pretrained weights for that architecture yet!\")\n"
     ]
    }
   ],
   "source": [
    "learn = language_model_learner(data_lm,TransformerXL, drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='4', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      75.00% [3/4 31:23:26<10:27:48]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.042905</td>\n",
       "      <td>3.068218</td>\n",
       "      <td>0.415210</td>\n",
       "      <td>10:29:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.004171</td>\n",
       "      <td>3.007097</td>\n",
       "      <td>0.422981</td>\n",
       "      <td>10:26:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.970160</td>\n",
       "      <td>2.963667</td>\n",
       "      <td>0.429145</td>\n",
       "      <td>10:26:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='56969' class='' max='126118', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      45.17% [56969/126118 4:33:30<5:31:58 2.9528]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 3.0682179927825928.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 2 with valid_loss value: 2.963667392730713.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(1e-3), callbacks=[SaveModelCallback(learn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (2076502 items)\n",
       "x: LMTextList\n",
       "[ ' \n",
       "  xbos xfld 1 xxmaj andorra , oficialmente xxmaj principado de xxmaj andorra ( ) , es un pequeño país soberano del suroeste de xxmaj europa . xxmaj constituido en xxmaj estado independiente , de derecho , democrático y social , cuya forma de gobierno es el _unk_ parlamentario . xxmaj su territorio está organizado en siete parroquias , con una población total de 78 \\ _unk_ habitantes . xxmaj su capital es xxmaj andorra la xxmaj vieja . ' , ' \n",
       "  xbos xfld 1 xxmaj tiene 468 \\ xa0 km² de extensión territorial y está situado en los xxmaj pirineos , entre xxmaj españa y xxmaj francia , con una altitud media de 1996 msnm . xxmaj limita por el sur con xxmaj españa — con las comarcas catalanas de xxmaj cerdaña , xxmaj alto xxmaj urgel y xxmaj pallars xxmaj _unk_ — y por el norte con xxmaj francia — con los departamentos de xxmaj _unk_ y xxmaj pirineos xxmaj orientales ( xxmaj _unk_ ' , ' \n",
       "  xbos xfld 1 xxmaj su sistema político es una democracia parlamentaria que tiene como jefes de xxmaj estado a los _unk_ de xxmaj andorra : el obispo de xxmaj urgel y el presidente de xxmaj francia ; y como jefe de xxmaj gobierno al presidente del xxmaj gobierno . xxmaj el idioma oficial es el catalán , lengua habitual de la mayoría de la población aunque el español representa la lengua habitual de alrededor de un tercio de la población . ' ],[ ' \n",
       "  xbos xfld 1 xxmaj no tiene ejército , pero sí un cuerpo de policía creado en 1931 . xxmaj en caso de emergencias o desastres naturales , como por ejemplo las inundaciones que ocurrieron en 1982 , la costumbre dictaba que se _unk_ al _unk_ , formado por los cabezas de familia con nacionalidad _unk_ , aunque es una práctica fuera de época y la policía del país gestiona todas las emergencias . xxmaj en caso de catástrofe grave , se _unk_ a ayuda española o francesa , según el tratado _unk_ de vecindad , amistad y cooperación . ' , ' \n",
       "  xbos xfld 1 xxmaj durante mucho tiempo pobre y aislado , consiguió una notable prosperidad desde la xxmaj segunda xxmaj guerra xxmaj mundial a través del turismo y , especialmente , por su condición de paraíso fiscal , condición que perdió para xxmaj españa desde el 10 de febrero de 2010 . ' , ' \n",
       "  xbos xfld 1 xxmaj es el _unk_ más grande de xxmaj europa . ' ],[ ' \n",
       "  xbos xfld 1 xxmaj el origen de la palabra \" xxmaj andorra \" es desconocido , aunque se han formulado diversas teorías al respecto . ' , ' \n",
       "  xbos xfld 1 xxmaj el escudo de armas del xxmaj principado de xxmaj andorra , cuyo lema es \" xxmaj virtus xxmaj unita xxmaj _unk_ \" , está formado por cuatro cuarteles ( dos por cada _unk_ ) . ' , ' \n",
       "  xbos xfld 1 xxmaj la letra del himno , titulado \" xxmaj el xxmaj gran xxmaj _unk_ \" ( en castellano , \" xxmaj el xxmaj gran xxmaj carlomagno \" ) , la escribió xxmaj joan xxmaj _unk_ i xxmaj _unk_ y la música fue compuesta por xxmaj enric xxmaj _unk_ xxmaj _unk_ . xxmaj el himno se adoptó oficialmente en 1914 y se interpretó por primera vez el 8 de septiembre de 1921 . ' ],[ ' \n",
       "  xbos xfld 1 xxmaj andorra fue gobernada de acuerdo con un sistema que remonta al xxmaj feudalismo . xxmaj según la tradición xxmaj carlomagno , el primer monarca del xxmaj sacro xxmaj imperio xxmaj romano xxmaj germánico concedió la independencia a este estado a cambio de ayuda para su lucha contra xxmaj al - xxmaj ándalus . xxmaj para acabar con las pugnas por el poder en 1278 se estableció un régimen de _unk_ mediante el cual el conde francés de xxmaj foix y el obispo de xxmaj urgel compartieron el gobierno de ese territorio . ' , ' \n",
       "  xbos xfld 1 xxmaj siglos después , la xxmaj revolución xxmaj francesa ( \" 1789 - 1799 ) \" , como heredera del territorio , renunció a sus derechos feudales sobre xxmaj andorra . xxmaj luego xxmaj napoleón xxmaj bonaparte volvió a aceptar su soberanía en 1806 por petición expresa de los _unk_ . ' , ' \n",
       "  xbos xfld 1 xxmaj durante el periodo del xxmaj mesolítico , pequeños grupos de humanos se asentaron en grutas próximas al xxmaj gran xxmaj _unk_ , como en la xxmaj _unk_ de la xxmaj _unk_ , y otros puntos del territorio como xxmaj pal , xxmaj la xxmaj _unk_ y xxmaj _unk_ , donde el 5 de junio de 2001 se encontró un sarcófago fabricado con losas de pizarra y que contenía restos humanos , brazaletes y recipientes de cerámica con alimentos . ' ],[ ' \n",
       "  xbos xfld 1 xxmaj debido a la fertilidad de las tierras , estos grupos las cultivaron y se establecieron definitivamente , recibiendo de los pueblos que pasaron por su territorio , la cultura del bronce al beneficiarse de los metales que había en xxmaj _unk_ . ' , ' \n",
       "  xbos xfld 1 xxmaj la primera referencia escrita sobre los _unk_ , se encuentra en la descripción que hizo el historiador griego xxmaj polibio sobre el paso de xxmaj aníbal por los xxmaj pirineos . xxmaj durante el siglo v , el xxmaj imperio romano sucumbe a los visigodos , que ocupan la xxmaj galia meridional y parte de xxmaj hispania . ' , ' \n",
       "  xbos xfld 1 xxmaj trescientos años después , los musulmanes _unk_ ese territorio , _unk_ la frontera con el xxmaj reino franco . xxmaj en el año 732 , xxmaj carlos xxmaj martel los derrota en la batalla de xxmaj tours , _unk_ su expansión hacia el xxmaj norte , aunque continúan asentados en los xxmaj pirineos . xxmaj posteriormente xxmaj carlomagno crea la xxmaj marca xxmaj hispánica y su sucesor xxmaj luis el xxmaj piadoso integra , en el año _unk_ , xxmaj andorra en el xxmaj imperio carolingio . ' ]\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts;\n",
       "\n",
       "Valid: LabelList (233628 items)\n",
       "x: LMTextList\n",
       "[ ' \n",
       "  xbos xfld 1 xxmaj la bandera de xxmaj andorra se adoptó oficialmente en 1866 y es tricolor vertical , azul , amarillo y rojo , la franja amarilla ligeramente más grande que las otras dos . xxmaj en el centro se muestra el escudo de xxmaj andorra , para diferenciarla de las banderas de xxmaj chad , xxmaj moldavia y xxmaj rumanía . ' , ' \n",
       "  xbos xfld 1 xxmaj en 1419 se creó el xxmaj consejo de la xxmaj tierra , formado por dos o tres representantes de las siete parroquias , con el objetivo de defender los intereses locales . xxmaj de nuevo volvió a _unk_ xxmaj fernando el xxmaj católico en 1512 , en su lucha contra los xxmaj albret de xxmaj navarra y contra los condes de xxmaj foix ; pero los reintegró un año más tarde a xxmaj germana de xxmaj foix , que había de ser su segunda esposa . xxmaj carlos v ratificó esta donación y renunció a todos sus derechos del xxmaj principado de xxmaj andorra , excepto al de nombrar obispo ( que ha sido retenido hasta la actualidad ) , que fue agregado a la corona francesa por xxmaj enrique xxup iii de xxmaj navarra y xxup iv de xxmaj francia en 1607 , y su hijo en 1620 xxmaj luis xxup xiii , lo declara unido a xxmaj francia hasta la xxmaj revolución , en el que xxmaj francia renunció a sus derechos temporalmente y el obispo de xxmaj urgel la gobernó para sí mismo , hasta que xxmaj napoleón volvió a aceptar la soberanía hasta 1814 , cuando fue liberada por xxmaj inglaterra y sus aliados y se formalizó un condominio entre xxmaj urgel y xxmaj francia , en el que se reconocía su independencia , y por eso los jefes de xxmaj estado serán el obispo de la xxmaj seo de xxmaj urgel y el ( en la actualidad , el presidente de la xxmaj república xxmaj francesa ) . ' , ' \n",
       "  xbos xfld 1 xxmaj el órgano ejecutivo está compuesto por un presidente del xxmaj gobierno y varios ministerios en número variable . ' ],[ ' \n",
       "  xbos xfld 1 xxmaj se sitúa en la península ibérica . xxmaj la superficie de xxmaj andorra , _unk_ en la península ibérica , es de 468 \\ xa0 km² . xxmaj su relieve es montañoso , con 65 picos de más de 2500 \\ xa0 m de altitud . xxmaj la montaña más alta es el pico de xxmaj _unk_ que , con una altitud de _unk_ \\ xa0msnm , está situado en la cordillera de los xxmaj pirineos , enclavado entre la frontera de xxmaj españa y xxmaj francia . xxmaj es una región de escarpados picos montañosos y estrechos valles por donde fluyen numerosos cursos de agua que se unen para formar los tres ríos principales : el río xxmaj _unk_ del xxmaj norte , el río xxmaj _unk_ de xxmaj oriente y el río xxmaj gran xxmaj _unk_ . xxmaj hay grandes extensiones boscosas de pinos y abedules debido al clima mediterráneo de alta montaña del que goza el país . xxmaj en 2004 , el valle de xxmaj _unk_ - xxmaj _unk_ - xxmaj _unk_ fue declarado xxmaj patrimonio de la xxmaj humanidad por la xxmaj unesco . ' , ' \n",
       "  xbos xfld 1 xxmaj la tasa de crecimiento demográfico es más alta que las de la mayoría de los estados europeos ; en 2008 se estimaba en el 2 % anual . xxmaj esta tasa de crecimiento demográfico es debida a una tasa de natalidad , estimada en 2008 , de 10,6 por cada 1.000 habitantes , una tasa de mortalidad de 5,6 por cada 1.000 habitantes , y una altísima tasa neta de migración de 14 migrantes por cada 1.000 habitantes . xxmaj la tasa de crecimiento anual había llegado a un máximo histórico del 7,7 % en 2003 , seguida del 6,4 % en 2004 , siendo xxmaj _unk_ la parroquia con la tasa más elevada . xxmaj la esperanza de vida de los _unk_ es la segunda más alta del mundo , estimada en _unk_ años ( _unk_ para los hombres y _unk_ para las mujeres ) . xxmaj la tasa de fertilidad es de _unk_ hijos por mujer . xxmaj en cuanto a la pirámide de edades , el 15,5 % de los _unk_ en 2008 tenían menos de 14 años , el _unk_ % entre 15 y 64 años , y el 12 % tenían 65 o más años . xxmaj la media de edad de los residentes _unk_ es de _unk_ años . ' , ' \n",
       "  xbos xfld 1 xxmaj tradicionalmente xxmaj andorra ha sido un país agrícola y ganadero aunque , desde la década de los 50 , el sector primario ha ido perdiendo importancia . xxmaj la producción agrícola está limitada , ya que sólo el 2 % de la tierra es cultivable , _unk_ tabaco en la casi totalidad de sus campos , y la principal actividad ganadera es la crianza bovina y equina , basadas en un sistema de explotación extensivo . xxmaj en el sector secundario predominan las industrias de transformación ( cigarrillos , puros y muebles ) e industrias primarias . xxmaj actualmente el sector terciario representa , según estimaciones , el 80 % del xxup pib _unk_ , siendo el turismo el sostén principal de la economía _unk_ . xxmaj nueve millones de personas la visitan anualmente , atraídas por su condición de paraíso fiscal , sus estaciones de esquí y el diferencial de precios en el comercio respecto de los países vecinos , aunque este último se ha erosionado recientemente mientras las economías francesa y española se han abierto , proporcionando una disponibilidad más amplia de bienes y tarifas más bajas . xxmaj en el año 2005 , el país recibió a _unk_ visitantes de los cuales _unk_ eran turistas y _unk_ excursionistas . xxmaj del total de visitantes , el _unk_ % eran españoles , el _unk_ % franceses y sólo un 3,0 % venían de otros países . ' ],[ ' \n",
       "  xbos xfld 1 xxmaj la primera emisora de radio comercial en emitir desde xxmaj andorra fue xxmaj ràdio xxmaj andorra que estuvo en activo desde 1939 hasta 1981 . xxmaj el 12 de octubre de 1989 , el \" xxmaj consell xxmaj general \" estableció la radio y la televisión como servicios públicos esenciales al crear la entidad gestora xxup _unk_ convirtiéndose , el 13 de abril de 2000 , en la sociedad pública \" xxmaj ràdio i xxmaj _unk_ d \\ ' xxmaj andorra , xxup s.a \" . ' , ' \n",
       "  xbos xfld 1 xxmaj además de la oferta pública , hay que añadir la existencia de 2 centros _unk_ que se rigen por el sistema español aunque mayoritariamente imparten las clases en catalán promoviendo , también , el uso del francés y el español . xxmaj para el curso escolar 2005 - 2006 , en el sistema francés estaban inscritos _unk_ alumnos , en el _unk_ _unk_ , y en el español _unk_ . ' , ' \n",
       "  xbos xfld 1 xxmaj en el 2004 , xxmaj andorra participó , por primera vez , en el xxmaj festival de la xxmaj canción de xxmaj eurovisión representada por xxmaj marta xxmaj _unk_ . xxmaj este hecho atrajo la atención de los medios de comunicación de xxmaj cataluña , ya que fue la primera canción _unk_ en catalán . xxmaj la canción fue eliminada en la semifinal , así como las composiciones del 2005 ( interpretada por xxmaj marian van de xxmaj wal ) , 2006 ( interpretada por xxmaj jenny ) , 2007 ( interpretada por xxmaj anonymous ) , 2008 ( interpretada por xxmaj gisela ) y 2009 ( interpretada por la cantante danesa xxmaj _unk_ xxmaj georgi ) . ' ],[ ' \n",
       "  xbos xfld 1 xxmaj el xxmaj futbol xxmaj club xxmaj andorra juega en xxmaj primera xxmaj territorial catalana de la xxmaj liga española de fútbol al estar inscrito en la xxmaj real xxmaj federación xxmaj española de xxmaj fútbol . ' , ' \n",
       "  xbos xfld 1 xxmaj integra el xxmaj mercosur — bloque del que fue fundador en 1991 — , la xxmaj unión de xxmaj naciones xxmaj sudamericanas ( xxmaj unasur ) , la xxmaj comunidad de xxmaj estados xxmaj latinoamericanos y xxmaj caribeños ( xxup _unk_ ) y la xxmaj organización de xxmaj estados xxmaj americanos ( xxup oea ) . ' , ' \n",
       "  xbos xfld 1 xxmaj el país tiene tres nombres oficiales establecidos desde 1860 por el artículo 35 la xxmaj constitución nacional , que pueden ser usados indistintamente : « xxmaj provincias xxmaj unidas del xxmaj río de la xxmaj plata » , « xxmaj república xxmaj argentina » y « xxmaj confederación xxmaj argentina » . xxmaj el más utilizado de los tres es el segundo , « xxmaj república xxmaj argentina » . xxmaj por _unk_ del sustantivo , suele decirse correctamente « la xxmaj argentina » . xxmaj sin embargo , está muy extendido el uso sin el artículo . ' ],[ ' \n",
       "  xbos xfld 1 xxmaj los primeros rastros de vida humana en este territorio corresponden a pueblos de un nivel cultural paleolítico que tres mil años atrás incorporaron los primeros aportes culturales _unk_ y neolíticos . xxmaj hasta la época de la conquista y de la colonización europea , el territorio argentino ha estado ocupado por diversos pueblos originarios , con diferentes organizaciones sociales que se pueden dividir en tres grupos principales : ' , ' \n",
       "  xbos xfld 1 xxmaj la población indígena sedentaria fue sometida a relaciones de dependencia permanente respecto de la población española . xxmaj aunque con el paso de las generaciones fue absorbida dentro una población étnicamente identificable como « criolla » , este proceso de _unk_ no fue total , como lo demuestra la participación de poblaciones del xxmaj noroeste del actual territorio argentino en el gran levantamiento indígena de 1780 con epicentro en el xxmaj cuzco , dirigido por el inca xxmaj túpac xxmaj amaru \\ xa0ii . ' , ' \n",
       "  xbos xfld 1 xxmaj la guerra de la independencia continuó hasta el año 1825 , pero se luchó preferentemente en la frontera norte y en el xxmaj perú . xxmaj mientras tanto , la xxmaj provincia xxmaj oriental fue invadida por el reino de xxmaj portugal , de quien pasó al xxmaj imperio del xxmaj brasil . xxmaj la consecuente xxmaj guerra del xxmaj brasil culminó con la xxmaj convención xxmaj preliminar de xxmaj paz de 1828 , que declaró independiente al territorio en disputa , con el nombre de xxmaj estado xxmaj oriental del xxmaj uruguay . xxmaj poco antes , en 1825 , el xxmaj alto xxmaj perú formó la xxmaj república de xxmaj bolivia , y al año siguiente le fue agregada la ciudad de xxmaj tarija y su jurisdicción . ' ]\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): TransformerXL(\n",
       "    (encoder): Embedding(60002, 410)\n",
       "    (pos_enc): PositionalEncoding()\n",
       "    (drop_emb): Dropout(p=0.05)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.05)\n",
       "          (drop_res): Dropout(p=0.05)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.05)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.05)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=60002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7ff7a887ed90>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05)\n",
       "      (drop_res): Dropout(p=0.05)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.05)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Embedding(60002, 410)\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=60002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('bestmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('encM2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[<function accuracy at 0x7ff7a887ed90>]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(learn.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.9477541, tensor(0.4313)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:OCR] *",
   "language": "python",
   "name": "conda-env-OCR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
