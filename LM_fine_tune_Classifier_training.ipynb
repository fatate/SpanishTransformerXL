{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "path =Path('/home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts') #path to where the dataset is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.es.Spanish at 0x7f5d8b9265f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttok=Tokenizer(lang='es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import fire\n",
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "def get_texts(df, n_lbls, lang='en'):\n",
    "    if len(df.columns) == 1:\n",
    "        labels = []\n",
    "        texts = f'\\n{BOS} {FLD} 1 ' + df[0].astype(str)\n",
    "    else:\n",
    "        labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "        texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "        for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls+1} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "\n",
    "    tok = Tokenizer(lang='es').process_all(partition_by_cores(texts,n_cpus=10000))\n",
    "    return tok, list(labels)\n",
    "\n",
    "\"\"\"\n",
    "def get_all(df, n_lbls, lang='en'):\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, n_lbls, lang=lang)\n",
    "        tok += tok_\n",
    "        labels += labels_\n",
    "    return tok, labels\n",
    "\"\"\"\n",
    "def get_all(df, n_lbls, lang='en'):\n",
    "    tok, labels = [], []\n",
    "    tok, labels = get_texts(df, n_lbls, lang=lang)\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(path / 'train.csv', header=None, sep='\\t')\n",
    "df_val = pd.read_csv(path / 'val.csv', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1], dtype='int64')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = path / 'tmp2'\n",
    "tmp_path.mkdir(exist_ok=True)\n",
    "tok_trn, trn_labels = get_all(df_trn, 1, lang='es_core_news_sm')\n",
    "tok_val, val_labels = get_all(df_val, 1, lang='es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Eslinga Std Plana Amarilla Cs/6 3 Tn X 2.5 Mts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Rodillera Con Barras Laterales, Unica Con Esta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Batería Lth Jet Ski Arctic Cat Tiger Shark 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Mecha Sds Plus Venturo 6x160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Kit Reparo Parcial Trambulador Corsa Meriva Mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  0     Eslinga Std Plana Amarilla Cs/6 3 Tn X 2.5 Mts\n",
       "1  0  Rodillera Con Barras Laterales, Unica Con Esta...\n",
       "2  1  Batería Lth Jet Ski Arctic Cat Tiger Shark 200...\n",
       "3  0                       Mecha Sds Plus Venturo 6x160\n",
       "4  0  Kit Reparo Parcial Trambulador Corsa Meriva Mo..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_path = path / 'tmp2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('xxmaj', 1568898), (\"'\", 500427), ('1', 266528), ('\\n ', 250872), ('xbos', 250872), ('xfld', 250872), (',', 182127), (']', 85788), ('[', 85787), ('de', 70381), ('-', 52783), ('/', 37035), ('xxup', 28065), ('bateria', 27549), ('para', 26738), ('batería', 19101), ('+', 16412), ('gas', 14938), ('kit', 13911), ('(', 13167), ('moto', 13072), ('2', 12608), ('lth', 12216), (')', 12147), ('c', 11169)]\n"
     ]
    }
   ],
   "source": [
    "min_freq=2\n",
    "max_vocab=60000\n",
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "print(freq.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40372\n"
     ]
    }
   ],
   "source": [
    "itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "print(len(itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(tmp_path / 'trn_ids.npy', trn_lm)\n",
    "np.save(tmp_path / 'val_ids.npy', val_lm)\n",
    "np.save(tmp_path / 'lbl_trn.npy', trn_labels)\n",
    "np.save(tmp_path / 'lbl_val.npy', val_labels)\n",
    "pickle.dump(itos, open(tmp_path / 'itosff.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwords=0\n",
    "for ii in range(len(trn_lm)):\n",
    "   wwords += len(trn_lm[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5684817"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.load(tmp_path / 'trn_ids.npy',allow_pickle=True)\n",
    "val_lm = np.load(tmp_path / 'val_ids.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = pickle.load(open(tmp_path / 'itosff.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_ids(path,vocab,trn_lm, val_lm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths to where the pretrained model and itos are\n",
    "\n",
    "learn = language_model_learner(data_lm, TransformerXL, pretrained_fnames=['/home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts/models/encM2', '/home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts/eswiki/tmp/itos',], drop_mult=0.3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.791656</td>\n",
       "      <td>2.692023</td>\n",
       "      <td>0.613253</td>\n",
       "      <td>04:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.342813</td>\n",
       "      <td>2.264520</td>\n",
       "      <td>0.644867</td>\n",
       "      <td>04:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.193646</td>\n",
       "      <td>2.146392</td>\n",
       "      <td>0.654350</td>\n",
       "      <td>04:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.141188</td>\n",
       "      <td>2.128421</td>\n",
       "      <td>0.656149</td>\n",
       "      <td>04:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 2.692023277282715.\n",
      "Better model found at epoch 1 with valid_loss value: 2.2645204067230225.\n",
      "Better model found at epoch 2 with valid_loss value: 2.1463921070098877.\n",
      "Better model found at epoch 3 with valid_loss value: 2.12842059135437.\n"
     ]
    }
   ],
   "source": [
    "learn.freeze()\n",
    "learn.fit_one_cycle(4, slice(1e-3), callbacks=[SaveModelCallback(learn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (85436 items)\n",
       "x: LMTextList\n",
       "[ ' \n",
       "  xbos xfld 1 xxmaj eslinga xxmaj std xxmaj plana xxmaj amarilla xxmaj cs / 6 3 xxmaj tn x 2.5 xxmaj mts ' , ' \n",
       "  xbos xfld 1 xxmaj rodillera xxmaj con xxmaj barras xxmaj laterales , xxmaj unica xxmaj con xxmaj estabilizador ' , ' \n",
       "  xbos xfld 1 xxmaj batería xxmaj lth xxmaj jet xxmaj ski xxmaj arctic xxmaj cat xxmaj tiger xxmaj shark 2002 640cc ' ],[ ' \n",
       "  xbos xfld 1 xxmaj mecha xxmaj sds xxmaj plus xxmaj venturo _unk_ ' , ' \n",
       "  xbos xfld 1 xxmaj kit xxmaj reparo xxmaj parcial xxmaj trambulador xxmaj corsa xxmaj meriva xxmaj montana 02 / c / nf ' , ' \n",
       "  xbos xfld 1 xxmaj fertilizante xxmaj agrícola xxmaj nitrato xxmaj de xxmaj cálcio xxmaj yara 5 xxmaj kg . ' ],[ ' \n",
       "  xbos xfld 1 xxmaj pistola xxmaj pressão xxmaj beeman 2004 xxup p17 5,5 + xxmaj red xxmaj dot 1x30 + xxmaj case ' , ' \n",
       "  xbos xfld 1 xxmaj dois xxmaj _unk_ xxmaj usado xxmaj originais xxmaj sedex xxmaj grátis ' , ' \n",
       "  xbos xfld 1 xxmaj bateria p xxmaj notebook xxmaj acer xxmaj aspire xxmaj timelinex 3820 t 4820 t xxmaj as10b73 ' ],[ ' \n",
       "  xbos xfld 1 xxmaj seachem xxmaj prime 4l xxmaj anticloro xxmaj cloro xxmaj remove xxmaj amônia xxmaj aquário ' , ' \n",
       "  xbos xfld 1 xxmaj rele xxmaj pisca xxmaj _unk_ / 89 ( _unk_ ) ' , ' \n",
       "  xbos xfld 1 xxmaj ácido xxmaj sulfonico 90 % - 10 xxmaj litros - xxmaj atacado + xxmaj frete xxmaj gratis ' ],[ ' \n",
       "  xbos xfld 1 xxmaj encendedor xxmaj zippo xxmaj price xxmaj fighter xxmaj blanco xxmaj con xxmaj diseño xxmaj zippo ' , ' \n",
       "  xbos xfld 1 xxmaj tornillo xxmaj fresada 12 x 2 xxmaj madera xxmaj caja x 200 - xxmaj perfecto ' , ' \n",
       "  xbos xfld 1 xxmaj batería xxmaj lth xxmaj moto xxmaj harley xxmaj davidson xxmaj xlch xxmaj series 1974 1000cc ' ]\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts;\n",
       "\n",
       "Valid: LabelList (16568 items)\n",
       "x: LMTextList\n",
       "[ ' \n",
       "  xbos xfld 1 xxmaj _unk_ 150w xxmaj automático xxmaj _unk_ xxmaj preto xxmaj cigarro xxmaj isqueiro xxmaj ali ' , ' \n",
       "  xbos xfld 1 xxmaj fita xxmaj dupla - face xxmaj de xxmaj papel xxmaj scotch ® 18mmx30 m ' , ' \n",
       "  xbos xfld 1 6 xxmaj multimassa xxmaj tedox 90 g xxmaj tapa xxmaj tudo xxmaj _unk_ xxmaj buraco xxmaj trinca xxmaj parede ' ],[ ' \n",
       "  xbos xfld 1 xxmaj party xxmaj popper , xxmaj cañón xxmaj de xxmaj confetti xxmaj sin xxmaj pólvora ' , ' \n",
       "  xbos xfld 1 xxmaj balão xxmaj fundo xxmaj redondo 2 xxmaj com xxmaj junta 14 / 20 125ml xxmaj ronialzi ' , ' \n",
       "  xbos xfld 1 xxmaj cleaning xxmaj capillary , 1.3 m xxmaj in xxmaj length ' ],[ ' \n",
       "  xbos xfld 1 xxmaj oleo xxmaj soluvel xxmaj tapmatic xxmaj _unk_ 5000ml ' , ' \n",
       "  xbos xfld 1 xxmaj bateria xxmaj gateway xxmaj nv51 xxmaj nv52 xxmaj _unk_ xxmaj _unk_ xxmaj _unk_ xxmaj as09a31 xxmaj as09a41 ' , ' \n",
       "  xbos xfld 1 xxmaj batería xxmaj lth xxmaj atv xxmaj utv xxmaj kawasaki xxmaj kaf620 2006 620cc ' ],[ ' \n",
       "  xbos xfld 1 xxmaj gas xxmaj butano xxmaj zippo 165gr - xxmaj cod 3810 ' , ' \n",
       "  xbos xfld 1 xxmaj ezgo _unk_ xxmaj heavy xxmaj duty xxmaj leaf xxmaj leaf xxmaj spring xxmaj _unk_ xxmaj leaf ' , ' \n",
       "  xbos xfld 1 xxmaj programa xxmaj de xxmaj lectura xxmaj _unk_ xxmaj para xxmaj bebés , xxmaj niños xxmaj pequeños , ' ],[ ' \n",
       "  xbos xfld 1 xxmaj tensor xxmaj tensores xxmaj galvanizado xxmaj de 5 mm 3 / 16 35 kg ' , ' \n",
       "  xbos xfld 1 xxmaj _unk_ xxmaj _unk_ xxmaj cuatro xxmaj función xxmaj camper xxmaj cuchillo xxmaj de xxmaj la xxmaj herram ' , ' \n",
       "  xbos xfld 1 xxmaj corrector xxmaj _unk_ . xxmaj girl xxmaj _unk_ xxmaj _unk_ xxmaj _unk_ ' ]\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): TransformerXL(\n",
       "    (encoder): Embedding(40372, 410)\n",
       "    (pos_enc): PositionalEncoding()\n",
       "    (drop_emb): Dropout(p=0.03)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=40372, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7fb9f0230d90>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Embedding(40372, 410)\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=40372, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('bestmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.955294</td>\n",
       "      <td>1.967302</td>\n",
       "      <td>0.677285</td>\n",
       "      <td>05:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.785473</td>\n",
       "      <td>1.826272</td>\n",
       "      <td>0.695249</td>\n",
       "      <td>05:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.665948</td>\n",
       "      <td>1.752568</td>\n",
       "      <td>0.705098</td>\n",
       "      <td>05:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.603894</td>\n",
       "      <td>1.741379</td>\n",
       "      <td>0.707101</td>\n",
       "      <td>05:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 1.9673024415969849.\n",
      "Better model found at epoch 1 with valid_loss value: 1.8262715339660645.\n",
      "Better model found at epoch 2 with valid_loss value: 1.7525678873062134.\n",
      "Better model found at epoch 3 with valid_loss value: 1.7413785457611084.\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(1e-3), callbacks=[SaveModelCallback(learn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (85436 items)\n",
       "x: LMTextList\n",
       "[ ' \n",
       "  xbos xfld 1 xxmaj eslinga xxmaj std xxmaj plana xxmaj amarilla xxmaj cs / 6 3 xxmaj tn x 2.5 xxmaj mts ' , ' \n",
       "  xbos xfld 1 xxmaj rodillera xxmaj con xxmaj barras xxmaj laterales , xxmaj unica xxmaj con xxmaj estabilizador ' , ' \n",
       "  xbos xfld 1 xxmaj batería xxmaj lth xxmaj jet xxmaj ski xxmaj arctic xxmaj cat xxmaj tiger xxmaj shark 2002 640cc ' ],[ ' \n",
       "  xbos xfld 1 xxmaj mecha xxmaj sds xxmaj plus xxmaj venturo _unk_ ' , ' \n",
       "  xbos xfld 1 xxmaj kit xxmaj reparo xxmaj parcial xxmaj trambulador xxmaj corsa xxmaj meriva xxmaj montana 02 / c / nf ' , ' \n",
       "  xbos xfld 1 xxmaj fertilizante xxmaj agrícola xxmaj nitrato xxmaj de xxmaj cálcio xxmaj yara 5 xxmaj kg . ' ],[ ' \n",
       "  xbos xfld 1 xxmaj pistola xxmaj pressão xxmaj beeman 2004 xxup p17 5,5 + xxmaj red xxmaj dot 1x30 + xxmaj case ' , ' \n",
       "  xbos xfld 1 xxmaj dois xxmaj _unk_ xxmaj usado xxmaj originais xxmaj sedex xxmaj grátis ' , ' \n",
       "  xbos xfld 1 xxmaj bateria p xxmaj notebook xxmaj acer xxmaj aspire xxmaj timelinex 3820 t 4820 t xxmaj as10b73 ' ],[ ' \n",
       "  xbos xfld 1 xxmaj seachem xxmaj prime 4l xxmaj anticloro xxmaj cloro xxmaj remove xxmaj amônia xxmaj aquário ' , ' \n",
       "  xbos xfld 1 xxmaj rele xxmaj pisca xxmaj _unk_ / 89 ( _unk_ ) ' , ' \n",
       "  xbos xfld 1 xxmaj ácido xxmaj sulfonico 90 % - 10 xxmaj litros - xxmaj atacado + xxmaj frete xxmaj gratis ' ],[ ' \n",
       "  xbos xfld 1 xxmaj encendedor xxmaj zippo xxmaj price xxmaj fighter xxmaj blanco xxmaj con xxmaj diseño xxmaj zippo ' , ' \n",
       "  xbos xfld 1 xxmaj tornillo xxmaj fresada 12 x 2 xxmaj madera xxmaj caja x 200 - xxmaj perfecto ' , ' \n",
       "  xbos xfld 1 xxmaj batería xxmaj lth xxmaj moto xxmaj harley xxmaj davidson xxmaj xlch xxmaj series 1974 1000cc ' ]\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts;\n",
       "\n",
       "Valid: LabelList (16568 items)\n",
       "x: LMTextList\n",
       "[ ' \n",
       "  xbos xfld 1 xxmaj _unk_ 150w xxmaj automático xxmaj _unk_ xxmaj preto xxmaj cigarro xxmaj isqueiro xxmaj ali ' , ' \n",
       "  xbos xfld 1 xxmaj fita xxmaj dupla - face xxmaj de xxmaj papel xxmaj scotch ® 18mmx30 m ' , ' \n",
       "  xbos xfld 1 6 xxmaj multimassa xxmaj tedox 90 g xxmaj tapa xxmaj tudo xxmaj _unk_ xxmaj buraco xxmaj trinca xxmaj parede ' ],[ ' \n",
       "  xbos xfld 1 xxmaj party xxmaj popper , xxmaj cañón xxmaj de xxmaj confetti xxmaj sin xxmaj pólvora ' , ' \n",
       "  xbos xfld 1 xxmaj balão xxmaj fundo xxmaj redondo 2 xxmaj com xxmaj junta 14 / 20 125ml xxmaj ronialzi ' , ' \n",
       "  xbos xfld 1 xxmaj cleaning xxmaj capillary , 1.3 m xxmaj in xxmaj length ' ],[ ' \n",
       "  xbos xfld 1 xxmaj oleo xxmaj soluvel xxmaj tapmatic xxmaj _unk_ 5000ml ' , ' \n",
       "  xbos xfld 1 xxmaj bateria xxmaj gateway xxmaj nv51 xxmaj nv52 xxmaj _unk_ xxmaj _unk_ xxmaj _unk_ xxmaj as09a31 xxmaj as09a41 ' , ' \n",
       "  xbos xfld 1 xxmaj batería xxmaj lth xxmaj atv xxmaj utv xxmaj kawasaki xxmaj kaf620 2006 620cc ' ],[ ' \n",
       "  xbos xfld 1 xxmaj gas xxmaj butano xxmaj zippo 165gr - xxmaj cod 3810 ' , ' \n",
       "  xbos xfld 1 xxmaj ezgo _unk_ xxmaj heavy xxmaj duty xxmaj leaf xxmaj leaf xxmaj spring xxmaj _unk_ xxmaj leaf ' , ' \n",
       "  xbos xfld 1 xxmaj programa xxmaj de xxmaj lectura xxmaj _unk_ xxmaj para xxmaj bebés , xxmaj niños xxmaj pequeños , ' ],[ ' \n",
       "  xbos xfld 1 xxmaj tensor xxmaj tensores xxmaj galvanizado xxmaj de 5 mm 3 / 16 35 kg ' , ' \n",
       "  xbos xfld 1 xxmaj _unk_ xxmaj _unk_ xxmaj cuatro xxmaj función xxmaj camper xxmaj cuchillo xxmaj de xxmaj la xxmaj herram ' , ' \n",
       "  xbos xfld 1 xxmaj corrector xxmaj _unk_ . xxmaj girl xxmaj _unk_ xxmaj _unk_ xxmaj _unk_ ' ]\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: /home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): TransformerXL(\n",
       "    (encoder): Embedding(40372, 410)\n",
       "    (pos_enc): PositionalEncoding()\n",
       "    (drop_emb): Dropout(p=0.03)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.03)\n",
       "          (drop_res): Dropout(p=0.03)\n",
       "          (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.03)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.03)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=40372, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7fba26385d90>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Embedding(40372, 410)\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=40372, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('bestmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.7436424, tensor(0.7070)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df_trn.iloc[:,df_names_to_idx(1, df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                    0\n",
       "1    Cartucho Canon Cli-8 C - Cian, Inyección De Tinta\n",
       "Name: 51305, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.iloc[51305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = TextClasDataBunch.from_df(path,train_df=df_trn,valid_df=df_val,tokenizer=ttok,vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn = text_classifier_learner(data_clas, TransformerXL,drop_mult=0.3)\n",
    "learn.load_encoder('enc')\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, TransformerXL,drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.148362</td>\n",
       "      <td>0.147007</td>\n",
       "      <td>0.948938</td>\n",
       "      <td>05:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.117641</td>\n",
       "      <td>0.115243</td>\n",
       "      <td>0.957719</td>\n",
       "      <td>05:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.092766</td>\n",
       "      <td>0.105471</td>\n",
       "      <td>0.963062</td>\n",
       "      <td>05:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.091154</td>\n",
       "      <td>0.099518</td>\n",
       "      <td>0.964198</td>\n",
       "      <td>05:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.14700716733932495.\n",
      "Better model found at epoch 1 with valid_loss value: 0.11524290591478348.\n",
      "Better model found at epoch 2 with valid_loss value: 0.10547143965959549.\n",
      "Better model found at epoch 3 with valid_loss value: 0.09951845556497574.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(4, slice(1e-3), callbacks=[SaveModelCallback(learn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (250871 items)\n",
       "x: TextList\n",
       "_unk_ xxmaj eslinga xxmaj std xxmaj plana xxmaj amarilla xxmaj cs / 6 3 xxmaj tn x 2.5 xxmaj mts,_unk_ xxmaj rodillera xxmaj con xxmaj barras xxmaj laterales , xxmaj unica xxmaj con xxmaj estabilizador,_unk_ xxmaj batería xxmaj lth xxmaj jet xxmaj ski xxmaj arctic xxmaj cat xxmaj tiger xxmaj shark 2002 640cc,_unk_ xxmaj mecha xxmaj sds xxmaj plus xxmaj venturo _unk_,_unk_ xxmaj kit xxmaj reparo xxmaj parcial xxmaj trambulador xxmaj corsa xxmaj meriva xxmaj montana 02 / c / nf\n",
       "y: CategoryList\n",
       "0,0,1,0,0\n",
       "Path: /home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts;\n",
       "\n",
       "Valid: LabelList (32568 items)\n",
       "x: TextList\n",
       "_unk_ xxmaj _unk_ 150w xxmaj automático xxmaj _unk_ xxmaj preto xxmaj cigarro xxmaj isqueiro xxmaj ali,_unk_ xxmaj fita xxmaj dupla - face xxmaj de xxmaj papel xxmaj scotch ® 18mmx30 m,_unk_ 6 xxmaj multimassa xxmaj tedox 90 g xxmaj tapa xxmaj tudo xxmaj _unk_ xxmaj buraco xxmaj trinca xxmaj parede,_unk_ xxmaj party xxmaj popper , xxmaj cañón xxmaj de xxmaj confetti xxmaj sin xxmaj pólvora,_unk_ xxmaj balão xxmaj fundo xxmaj redondo 2 xxmaj com xxmaj junta 14 / 20 125ml xxmaj ronialzi\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: /home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): TransformerXL(\n",
       "      (encoder): Embedding(40372, 410)\n",
       "      (pos_enc): PositionalEncoding()\n",
       "      (drop_emb): Dropout(p=0.03)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.03)\n",
       "            (drop_res): Dropout(p=0.03)\n",
       "            (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace)\n",
       "              (2): Dropout(p=0.03)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.03)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.03)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f5da1657378>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/ubuntu/fastai2/fastai/courses/dl2/imdb_scripts'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(40372, 410)\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.03)\n",
       "      (drop_res): Dropout(p=0.03)\n",
       "      (ln): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace)\n",
       "        (2): Dropout(p=0.03)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.03)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm(torch.Size([410]), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.03)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('bestmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='8', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/8 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2812' class='' max='3919', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      71.75% [2812/3919 03:39<01:26 0.0864]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8, slice(1e-5,1e-3), callbacks=[SaveModelCallback(learn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.102217995, tensor(0.9669)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:OCR] *",
   "language": "python",
   "name": "conda-env-OCR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
